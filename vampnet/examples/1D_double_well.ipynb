{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeptime\n",
      "  Using cached deeptime-0.4.2.tar.gz (390 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from deeptime) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from deeptime) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from deeptime) (1.1.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from deeptime) (1.22.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->deeptime) (1.1.0)\n",
      "Building wheels for collected packages: deeptime\n",
      "  Building wheel for deeptime (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deeptime: filename=deeptime-0.4.2-cp38-cp38-macosx_10_16_x86_64.whl size=1494553 sha256=5b563999532a19b0c2c336d083fa934106a6c6924a789bdeee19986d56dc31b9\n",
      "  Stored in directory: /Users/maya/Library/Caches/pip/wheels/f9/0a/31/60abbe97328697f04e503ee0cfc7259f8416c79b22bc5f4f3f\n",
      "Successfully built deeptime\n",
      "Installing collected packages: deeptime\n",
      "Successfully installed deeptime-0.4.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deeptime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement vampnet (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for vampnet\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vampnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vampnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/maya/Desktop/lab/deeptime/deeptime/vampnet/examples/1D_double_well.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maya/Desktop/lab/deeptime/deeptime/vampnet/examples/1D_double_well.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maya/Desktop/lab/deeptime/deeptime/vampnet/examples/1D_double_well.ipynb#ch0000003?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maya/Desktop/lab/deeptime/deeptime/vampnet/examples/1D_double_well.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mvampnet\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maya/Desktop/lab/deeptime/deeptime/vampnet/examples/1D_double_well.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvampnet\u001b[39;00m \u001b[39mimport\u001b[39;00m data_generator\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maya/Desktop/lab/deeptime/deeptime/vampnet/examples/1D_double_well.ipynb#ch0000003?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vampnet'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import vampnet\n",
    "from vampnet import data_generator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, BatchNormalization, concatenate\n",
    "from keras import optimizears\n",
    "import tensorflow as tf\n",
    "from keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 50000 frames and energy values\n",
    "datapoints = int(5e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_whole = data_generator.get_asymmetric_double_well_data(datapoints)\n",
    "# To fit the dataformat\n",
    "traj_whole = np.expand_dims(traj_whole, 1)\n",
    "traj_data_points, input_size = traj_whole.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,5,500)\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.ylim(-15,10)\n",
    "plt.xlim(-1,5)\n",
    "plt.plot(x,data_generator.asymmetric_double_well_energy(x), lw = 2)\n",
    "plt.xlabel('Position x / a.u.', fontsize = 16)\n",
    "plt.ylabel('Pot. energy / a.u.', fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "\n",
    "plt.yticks(fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Hyperparameters\n",
    "\n",
    "# Tau, how much is the timeshift of the two datasets\n",
    "tau = 1\n",
    "\n",
    "# Batch size for Stochastic Gradient descent\n",
    "batch_size = 2048\n",
    "\n",
    "# Which trajectory points percentage is used as training\n",
    "train_ratio = 0.9\n",
    "\n",
    "# How many hidden layers the network has\n",
    "network_depth = 4\n",
    "\n",
    "# Width of every layer\n",
    "layer_width = 20\n",
    "nodes = [layer_width]*network_depth\n",
    "# Learning rate used for the ADAM optimizer\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# How many output states the network has\n",
    "output_size = 5\n",
    "\n",
    "# Iteration over the training set in the fitting process\n",
    "nb_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "vamp = vampnet.VampnetTools(epsilon = epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle trajectory and lagged trajectory together\n",
    "length_data = traj_data_points - tau\n",
    "\n",
    "traj_ord= traj_whole[:length_data]\n",
    "traj_ord_lag = traj_whole[tau:length_data+tau]\n",
    "\n",
    "indexes = np.arange(length_data)\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "\n",
    "\n",
    "traj = traj_ord[indexes]\n",
    "traj_lag = traj_ord_lag[indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for tensorflow usage\n",
    "length_train = int(np.floor(length_data * train_ratio))\n",
    "length_vali = length_data - length_train\n",
    "\n",
    "traj_data_train = traj[:length_train]\n",
    "traj_data_train_lag = traj_lag[:length_train]\n",
    "\n",
    "traj_data_valid = traj[length_train:]\n",
    "traj_data_valid_lag = traj_lag[length_train:]\n",
    "\n",
    "#Data used for states ordering\n",
    "X1 = traj_ord[:length_data].astype('float32')\n",
    "X2 = traj_ord_lag[:length_data].astype('float32')\n",
    "\n",
    "# Input of the first network\n",
    "X1_train = traj_data_train.astype('float32')\n",
    "X2_train  = traj_data_train_lag.astype('float32')\n",
    "\n",
    "# Input for validation\n",
    "X1_vali = traj_data_valid.astype('float32')\n",
    "X2_vali = traj_data_valid_lag.astype('float32')\n",
    "\n",
    "# Needs a Y-train set which we dont have.\n",
    "Y_train = np.zeros((length_train,2*output_size)).astype('float32')\n",
    "Y_vali = np.zeros((length_vali,2*output_size)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in globals():\n",
    "    del model\n",
    "    clear_session()\n",
    "# Build the model\n",
    "Data_X = Input(shape = (input_size,))\n",
    "Data_Y = Input(shape = (input_size,))\n",
    "\n",
    "# A batch normalization layer improves convergence speed\n",
    "#     bn_layer = BatchNormalization()\n",
    "bn_layer = Activation('linear')\n",
    "\n",
    "# Instance layers and assign them to the two lobes of the network\n",
    "dense_layers = [Dense(node, activation = 'relu',)\n",
    "                for node in nodes]\n",
    "\n",
    "lx_branch = bn_layer(Data_X)\n",
    "rx_branch = bn_layer(Data_Y)\n",
    "\n",
    "for i, layer in enumerate(dense_layers):\n",
    "\n",
    "    lx_branch = dense_layers[i](lx_branch)\n",
    "    rx_branch = dense_layers[i](rx_branch)\n",
    "\n",
    "\n",
    "# Add a softmax output layer.\n",
    "# Should be replaced with a linear activation layer if\n",
    "# the outputs of the network cannot be interpreted as states\n",
    "softmax = Dense(output_size, activation='softmax')\n",
    "\n",
    "lx_branch = softmax(lx_branch)\n",
    "rx_branch = softmax(rx_branch)\n",
    "\n",
    "# Merge both networks to train both at the same time\n",
    "merged = concatenate([lx_branch, rx_branch])\n",
    "\n",
    "# Initialize the model and the optimizer, and compile it with\n",
    "# the loss and metric functions from the VAMPnets package\n",
    "model = Model(inputs = [Data_X, Data_Y], outputs = merged)\n",
    "#     model.summary()\n",
    "# Compile it with our own loss-function\n",
    "adam = optimizers.adam(lr = learning_rate)\n",
    "\n",
    "\n",
    "# Pretraining with VAMP with 'symmetrized' matrices yields a bad approximation of the \n",
    "# eigenvectors per se, but improves the 'readability' of the states identified by VAMP-2\n",
    "# which would otherwise be difficult to interprete.\n",
    "\n",
    "\n",
    "# IMPORTANT: the function vamp.loss_VAMP2_autograd can only be used with tensorflow 1.6 or more recent.\n",
    "# For older versions of TF, use the function vamp.loss_VAMP2\n",
    "\n",
    "losses = [\n",
    "    vamp._loss_VAMP_sym,\n",
    "    vamp.loss_VAMP2,\n",
    "]\n",
    "\n",
    "valid_metric = np.zeros((len(losses), nb_epoch))\n",
    "train_metric = np.zeros((len(losses), nb_epoch))\n",
    "\n",
    "for l_index, loss in enumerate(losses):\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = loss, metrics = [vamp.metric_VAMP])\n",
    "    \n",
    "    hist = model.fit([X1_train, X2_train], Y_train ,batch_size=batch_size, epochs=nb_epoch, verbose=0,\n",
    "                     validation_data=([X1_vali, X2_vali], Y_vali))\n",
    "    \n",
    "    temp = model.predict([traj_ord, traj_ord_lag], batch_size=np.shape(X1_vali)[0])\n",
    "    \n",
    "    x_a = temp[:,:output_size]\n",
    "\n",
    "\n",
    "    X_Validation = np.squeeze(traj_ord)\n",
    "    for i in range(output_size):\n",
    "        plt.scatter(X_Validation, x_a[:,i], label= 'state '+str(i))\n",
    "    plt.title('State probabilities')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    states_prob_meanfree = x_a  - np.mean(x_a, axis=0)\n",
    "    tau_msm = 5\n",
    "    K_smt = vamp.estimate_koopman_op(states_prob_meanfree, tau_msm)\n",
    "\n",
    "    K_eigvals, K_eigvec = np.linalg.eig(np.real(K_smt))\n",
    "\n",
    "    index = np.argmax(np.real(K_eigvals))\n",
    "    real_eigfunc = states_prob_meanfree @ np.real(K_eigvec[:,index])\n",
    "\n",
    "    plt.scatter(X_Validation, real_eigfunc)\n",
    "    plt.title('Eigenvector')\n",
    "    plt.show()\n",
    "\n",
    "    valid_metric[l_index] = np.array(hist.history['val_metric_VAMP'])\n",
    "    train_metric[l_index] = np.array(hist.history['metric_VAMP'])\n",
    "\n",
    "valid_metric = np.reshape(valid_metric, (-1))\n",
    "train_metric = np.reshape(train_metric, (-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_metric, label = 'Training')\n",
    "plt.legend()\n",
    "plt.plot(valid_metric, label = 'Validation')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the input trajectory using the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_prob = model.predict([traj_ord, traj_ord_lag])[:, :output_size]\n",
    "\n",
    "# Order the output states based on their population\n",
    "coor_pred = np.argmax(states_prob, axis = 1)\n",
    "indexes = [np.where(coor_pred == np.multiply(np.ones_like(coor_pred), n)) for n in range(output_size)]\n",
    "states_num = [len(i[0]) for i in indexes]\n",
    "states_order = np.argsort(states_num).astype('int')[::-1]\n",
    "\n",
    "pred_ord = states_prob[:,states_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the population of the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_states_pie_chart():\n",
    "    coors = []\n",
    "    maxi = np.max(pred_ord, axis= 1)\n",
    "\n",
    "    for i in range(output_size):\n",
    "        coors.append(len(np.where(pred_ord[:,i] == maxi)[0]))\n",
    "        \n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(np.array(coors), autopct='%1.2f%%', startangle=90)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    print('States population: '+str(np.array(coors)/len(maxi)*100)+'%')\n",
    "    plt.show()\n",
    "\n",
    "print_states_pie_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the implied timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tau = 15\n",
    "lag = np.arange(1, max_tau, 1)\n",
    "its = vamp.get_its(pred_ord, lag)\n",
    "vamp.plot_its(its, lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapman-Kolmogorov test for the estimated koopman operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 16\n",
    "tau_msm = 1\n",
    "predicted, estimated = vamp.get_ck_test(pred_ord, steps, tau_msm)\n",
    "vamp.plot_ck_test(predicted, estimated, output_size, steps, tau_msm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
